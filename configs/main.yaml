# Configurations for MyModel (encoder.py)
model:
  tau_initial: 0.5 # Initial value for tau
  vit: "google/vit-base-patch16-224" # Pre-trained ViT model
  clip_model_name: "openai/clip-vit-base-patch32" # Pre-trained CLIP model
  max_length: 77 # Maximum sequence length for token generation
  hard: False # Use hard sampling in gumbel softmax
  dim: -1 # Dimension for operations in MyModel

# Training configurations (train_model.py)
training:
  learning_rate: 0.001 # Learning rate for optimizer
  batch_size: 1 # Batch size for data loading
  num_epochs: 10 # Number of training epochs
  pre_trained_diffusion_model: "CompVis/stable-diffusion-v1-4" # Pre-trained diffusion model
  resize_dataloader: 256 # Resize parameter for DataLoader
  num_images: 100 # Number of images to train on
# BLIP Model and Processor configurations
inference:
  pretrained_blip_model: "Salesforce/blip-image-captioning-large" # Pre-trained BLIP model
  max_length: 50
  min_length: 20
  num_beams: 5
  output_path: "outputs/figures/inference_image.jpg" # path for image generation
  img_path: "https://farm1.staticflickr.com/123/411486338_558f63a3eb_z.jpg" # URL or path of the original image you wish to use
# Dataset configurations
dataset:
  cifar10_path: "datasets"
# Model saving configurations
model_checkpoint:
  path: "logs"
  filename: "model_checkpoints.pth"
